# -*- coding: utf-8 -*-
"""Mental-Healthcare-chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oSO2xhrkueAuo0phBrB3Hx43Aur8o_gD
"""

!pip install langchain_groq langchain_core langchain_community

from langchain_groq import ChatGroq

llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)

result = llm.invoke("Define healthcare")
print(result.content)

!pip install pypdf

!pip install chromadb

!pip install sentence_transformers

import os
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader,DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama-3.3-70b-versatile"
)
    return llm

def create_vector_db():
  loader = DirectoryLoader("/content/data",glob="*.pdf",loader_cls=PyPDFLoader)
  documents = loader.load()
  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)
  texts = text_splitter.split_documents(documents)
  embeddings = HuggingFaceBgeEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
  vector_db = Chroma.from_documents(texts,embeddings,persist_directory="./chroma_db")
  vector_db.persist()
  print("ChromaDB created and data saved")
  return vector_db

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a compassionate mental health chatbot. Respond thoughtfully.

{context}
User: {question}
Chatbot:"""
    PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

def main():
    print("Initializing Chatbot.......")
    llm = initialize_llm()
    db_path = "/content/chroma_db"


    if not os.path.exists(db_path):
        vector_db = create_vector_db()
    else:
        embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

    qa_chain = setup_qa_chain(vector_db, llm)

    while True:
        query = input("\nHuman: ")
        if query.lower() == "exit":
            print("Chatbot: Take care of yourself, Goodbye!")
            break
        response = qa_chain.run(query)
        print(f"Chatbot: {response}")

if __name__ == "__main__":
    main()

#create_vector_db()

#Training the dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# Load tokenizer and model (e.g., DistilBERT for classification)
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Load a small dataset (you can replace this with your mental health Q&A data)
dataset = load_dataset("imdb", split='train[:1%]')
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

dataset = dataset.map(tokenize, batched=True)
dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=2,
    per_device_train_batch_size=8,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="no"
)

# Trainer with built-in loss logging
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

# Train the model
trainer.train()


!pip install gradio

import os
import gradio as gr
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter


def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama-3.3-70b-versatile"
    )
    return llm


def create_vector_db():
    loader = DirectoryLoader("/content/data", glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
    )
    texts = text_splitter.split_documents(documents)

    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
    vector_db.persist()

    print("‚úÖ ChromaDB created and data saved")
    return vector_db


def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a compassionate mental health chatbot. Respond thoughtfully.

{context}
User: {question}
Chatbot:"""
    PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain


print("Initializing Chatbot.......")
db_path = "/content/chroma_db"
llm = initialize_llm()

if not os.path.exists(db_path):
    vector_db = create_vector_db()
else:
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

qa_chain = setup_qa_chain(vector_db, llm)


def chatbot_response(user_input, history = []):
    if not user_input.strip():
        return "Please provide a valid input"
    try:
        response = qa_chain.run(user_input)
        return response
    except Exception as e:
        return f"‚ö†Ô∏è Error: {str(e)}"

with gr.Blocks(css="""
body {
    background: linear-gradient(to right, #a8e6cf, #dcedc1);  /* Shades of green */
    font-family: 'Segoe UI', sans-serif;
}

h1, h2, h3 {
    color: #ff6f61;  /* Vibrant coral */
}

p, label {
    color: #ff9800;  /* Bright orange */
}

.gr-button {
    background-color: #388e3c !important;  /* Rich green */
    color: white !important;
    border-radius: 10px;
}

.gr-button:hover {
    background-color: #1b5e20 !important;  /* Darker green */
}
""") as app:


    chatbot = gr.ChatInterface(
        fn=chatbot_response,
        title="üß† Mental Health Chatbot",
        description="Ask anything related to mental wellness. I‚Äôm here to support you üíõ"
    )

app.launch()

